#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Ù…Ø³ØªØ®Ø±Ø¬ Ø§Ù„Ø¢ÙŠØ§Øª Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ Ù…Ù† Ù…Ù„ÙØ§Øª SVG Ùˆ PDF Ù…Ø¹ Ø¯Ø¹Ù… Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ
ÙŠØ³ØªØ®Ø¯Ù… Ù…Ø¯ÙŠØ± Ø§Ù„Ø®Ø· Ø§Ù„Ù…ØªØ·ÙˆØ± ÙˆÙ…Ø¹Ø§Ù„Ø¬ Ø§Ù„Ù†ØµÙˆØµ Ù…Ø¹ ØªØ­Ù„ÙŠÙ„ AI Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø£ÙØ¶Ù„ Ø¯Ù‚Ø©

Ø§Ù„Ù…Ù…ÙŠØ²Ø§Øª:
- Ø¯Ø¹Ù… Ù…ØªØ·ÙˆØ± Ù„Ù…Ù„ÙØ§Øª SVG Ù…Ø¹ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØªØ­ÙˆÙŠÙ„Ø§Øª Ø§Ù„Ù…Ø¹Ù‚Ø¯Ø©
- Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ù† PDF Ù…Ø¹ Ø¯Ø¹Ù… Ø§Ù„Ø·Ø¨Ù‚Ø§Øª Ø§Ù„Ù…ØªØ¹Ø¯Ø¯Ø©
- ØªØ­Ù„ÙŠÙ„ Ø¨Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Groq API
- Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ§Øª Ù…Ø·Ø§Ø¨Ù‚Ø© Ø°ÙƒÙŠØ© Ù…ØªØ¹Ø¯Ø¯Ø© Ø§Ù„Ù…Ø³ØªÙˆÙŠØ§Øª
- Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…ØªÙˆØ§Ø²ÙŠØ© Ù„Ù„Ø£Ø¯Ø§Ø¡ Ø§Ù„Ø¹Ø§Ù„ÙŠ
- ØªÙ‚Ø§Ø±ÙŠØ± Ù…ÙØµÙ„Ø© ÙˆØ¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø´Ø§Ù…Ù„Ø©
"""

import os
import json
import logging
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Any
from datetime import datetime
import xml.etree.ElementTree as ET
import fitz  # PyMuPDF
import pandas as pd
import numpy as np
from dataclasses import dataclass, field
from concurrent.futures import ThreadPoolExecutor, as_completed
import argparse
from tqdm import tqdm
import re
from collections import defaultdict
import io

# Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ù…ÙƒÙˆÙ†Ø§Øª Ø§Ù„Ù…Ø­Ù„ÙŠØ©
from gui.Agent.font_manager import AdvancedFontManager
from gui.Agent.text_processor import AdvancedTextProcessor
from gui.Agent.ai_analyzer import AIAnalyzer, PageAnalysis, TextRegion

@dataclass
class ExtractedText:
    """Ù†Øµ Ù…Ø³ØªØ®Ø±Ø¬ Ù…Ø¹ Ù…Ø¹Ù„ÙˆÙ…Ø§ØªÙ‡"""
    text: str
    bbox: List[float]  # [x1, y1, x2, y2]
    page: int
    source: str  # 'svg' Ø£Ùˆ 'pdf'
    line_number: Optional[int] = None
    confidence: float = 1.0
    metadata: Dict[str, Any] = field(default_factory=dict)
    
@dataclass
class MatchedAyah:
    """Ø¢ÙŠØ© Ù…Ø·Ø§Ø¨Ù‚Ø© Ù…Ø¹ Ù…Ø¹Ù„ÙˆÙ…Ø§ØªÙ‡Ø§"""
    page: int
    sura_no: int
    sura_name_ar: str
    sura_name_en: str
    aya_no: int
    text_uthmani: str
    text_extracted: str
    text_clean: str
    similarity: float
    bbox: List[float]
    source: str
    extraction_method: str
    confidence: float
    metadata: Dict[str, Any] = field(default_factory=dict)
    
class AyatExtractor:
    """Ù…Ø³ØªØ®Ø±Ø¬ Ø§Ù„Ø¢ÙŠØ§Øª Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ Ù…Ø¹ Ø¯Ø¹Ù… Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ"""
    
    def __init__(self, config: Dict[str, Any]):
        """
        ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬
        
        Args:
            config: Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ØªØ´ØºÙŠÙ„
        """
        self.config = config
        self.setup_logging()
        
        # ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù…ÙƒÙˆÙ†Ø§Øª
        self.font_manager = AdvancedFontManager(
            config['font_file'],
            cache_dir=config.get('cache_dir', 'font_cache'),
            logger=self.logger
        )
        
        self.text_processor = AdvancedTextProcessor(
            font_manager=self.font_manager,
            logger=self.logger
        )
        
        # ØªÙ‡ÙŠØ¦Ø© Ù…Ø­Ù„Ù„ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø¥Ù† ÙƒØ§Ù† Ù…ÙØ¹Ù„Ø§Ù‹
        self.ai_analyzer = None
        if config.get('use_ai', False) and config.get('groq_api_key'):
            try:
                self.ai_analyzer = AIAnalyzer(
                    api_key=config['groq_api_key'],
                    cache_dir=config.get('ai_cache_dir', 'ai_cache'),
                    logger=self.logger
                )
                self.logger.info("âœ… ØªÙ… ØªÙØ¹ÙŠÙ„ Ù…Ø­Ù„Ù„ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ")
            except Exception as e:
                self.logger.warning(f"âš ï¸ ÙØ´Ù„ ØªÙØ¹ÙŠÙ„ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ: {e}")
                self.ai_analyzer = None
        
        # ØªØ­Ù…ÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØµØ­Ù
        self.hafs_data = self._load_hafs_data()
        
        # Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
        self.stats = defaultdict(int)
        
    def setup_logging(self):
        """Ø¥Ø¹Ø¯Ø§Ø¯ Ù†Ø¸Ø§Ù… Ø§Ù„ØªØ³Ø¬ÙŠÙ„"""
        log_level = logging.DEBUG if self.config.get('verbose') else logging.INFO
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø¬Ù„Ø¯ Ø§Ù„Ø³Ø¬Ù„Ø§Øª
        log_dir = Path(self.config.get('log_dir', 'logs'))
        log_dir.mkdir(exist_ok=True)
        
        # Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù…Ø³Ø¬Ù„
        logging.basicConfig(
            level=log_level,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(
                    log_dir / f'ayat_extractor_{datetime.now():%Y%m%d_%H%M%S}.log',
                    encoding='utf-8'
                ),
                logging.StreamHandler()
            ]
        )
        
        self.logger = logging.getLogger(__name__)
        self.logger.info("=" * 60)
        self.logger.info("Ù…Ø³ØªØ®Ø±Ø¬ Ø§Ù„Ø¢ÙŠØ§Øª Ø§Ù„Ù…ØªØ·ÙˆØ± - Ø§Ù„Ø¥ØµØ¯Ø§Ø± 3.0")
        self.logger.info("=" * 60)
        
    def _load_hafs_data(self) -> pd.DataFrame:
        """ØªØ­Ù…ÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØµØ­Ù"""
        csv_path = self.config['csv_path']
        
        try:
            # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            df = pd.read_csv(csv_path)
            
            # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            df['aya_text'] = df['aya_text'].fillna('')
            df['aya_text_emlaey'] = df['aya_text_emlaey'].fillna('')
            
            self.logger.info(f"âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ {len(df)} Ø¢ÙŠØ© Ù…Ù† Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØµØ­Ù")
            
            # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø£ÙˆÙ„ÙŠØ© Ù„Ù„Ù†ØµÙˆØµ
            self.logger.info("Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ù…Ø±Ø¬Ø¹ÙŠØ©...")
            df['text_normalized'] = df['aya_text'].apply(
                lambda x: self.text_processor.normalize_text(x, level='basic')
            )
            df['text_clean'] = df['aya_text'].apply(
                lambda x: self.text_processor.clean_text(x)
            )
            
            return df
            
        except Exception as e:
            self.logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØµØ­Ù: {e}")
            raise
            
    def extract_from_svg_with_ai(self, svg_path: str) -> List[ExtractedText]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†ØµÙˆØµ Ù…Ù† SVG Ù…Ø¹ Ø¯Ø¹Ù… Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ"""
        # Ø§Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ØªÙ‚Ù„ÙŠØ¯ÙŠ Ø£ÙˆÙ„Ø§Ù‹
        traditional_texts = self.extract_from_svg(svg_path)
        
        # Ø¥Ø°Ø§ ÙƒØ§Ù† AI Ù…ÙØ¹Ù„Ø§Ù‹ØŒ Ù†Ø­Ø³Ù† Ø§Ù„Ù†ØªØ§Ø¦Ø¬
        if self.ai_analyzer and self.config.get('ai_enhance_svg', True):
            try:
                # ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØµÙØ­Ø© Ø¨Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ
                page_analysis = self.ai_analyzer.analyze_page(
                    svg_path,
                    ['text_regions', 'ayah_detection']
                )
                
                # Ø¯Ù…Ø¬ Ù†ØªØ§Ø¦Ø¬ AI Ù…Ø¹ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ØªÙ‚Ù„ÙŠØ¯ÙŠØ©
                enhanced_texts = self._merge_ai_results(
                    traditional_texts,
                    page_analysis
                )
                
                self.stats['ai_enhanced_pages'] += 1
                return enhanced_texts
                
            except Exception as e:
                self.logger.warning(f"ÙØ´Ù„ ØªØ­Ø³ÙŠÙ† AI Ù„Ù„ØµÙØ­Ø©: {e}")
                return traditional_texts
        
        return traditional_texts
    
    def _merge_ai_results(self, traditional_texts: List[ExtractedText], 
                         ai_analysis: PageAnalysis) -> List[ExtractedText]:
        """Ø¯Ù…Ø¬ Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ØªÙ‚Ù„ÙŠØ¯ÙŠ Ù…Ø¹ ØªØ­Ù„ÙŠÙ„ AI"""
        merged = []
        used_ai_regions = set()
        
        # Ù…Ø·Ø§Ø¨Ù‚Ø© Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„ØªÙ‚Ù„ÙŠØ¯ÙŠØ© Ù…Ø¹ Ù…Ù†Ø§Ø·Ù‚ AI
        for trad_text in traditional_texts:
            best_match = None
            best_overlap = 0.0
            
            for idx, ai_region in enumerate(ai_analysis.text_regions):
                if idx in used_ai_regions:
                    continue
                    
                # Ø­Ø³Ø§Ø¨ Ø§Ù„ØªØ¯Ø§Ø®Ù„ Ø¨ÙŠÙ† Ø§Ù„Ù…Ù†Ø§Ø·Ù‚
                overlap = self._calculate_bbox_overlap(
                    trad_text.bbox,
                    ai_region.bbox
                )
                
                if overlap > best_overlap and overlap > 0.5:
                    best_overlap = overlap
                    best_match = (idx, ai_region)
            
            if best_match:
                idx, ai_region = best_match
                used_ai_regions.add(idx)
                
                # Ø¯Ù…Ø¬ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª
                enhanced_text = ExtractedText(
                    text=trad_text.text,  # Ù†Ø«Ù‚ Ø¨Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬
                    bbox=ai_region.bbox,   # Ù†Ø³ØªØ®Ø¯Ù… Ø¥Ø­Ø¯Ø§Ø«ÙŠØ§Øª AI Ø§Ù„Ø£Ø¯Ù‚
                    page=trad_text.page,
                    source='svg_ai_enhanced',
                    confidence=max(trad_text.confidence, ai_region.confidence),
                    metadata={
                        **trad_text.metadata,
                        'ai_text_type': ai_region.text_type,
                        'ai_font_type': ai_region.font_type,
                        'ai_confidence': ai_region.confidence
                    }
                )
                merged.append(enhanced_text)
            else:
                # Ù„Ù… Ù†Ø¬Ø¯ Ù…Ø·Ø§Ø¨Ù‚Ø© AIØŒ Ù†Ø³ØªØ®Ø¯Ù… Ø§Ù„Ù†Øµ Ø§Ù„ØªÙ‚Ù„ÙŠØ¯ÙŠ
                merged.append(trad_text)
        
        # Ø¥Ø¶Ø§ÙØ© Ù…Ù†Ø§Ø·Ù‚ AI Ø§Ù„ØªÙŠ Ù„Ù… ØªØ·Ø§Ø¨Ù‚ Ø£ÙŠ Ù†Øµ ØªÙ‚Ù„ÙŠØ¯ÙŠ
        for idx, ai_region in enumerate(ai_analysis.text_regions):
            if idx not in used_ai_regions and ai_region.text_type == 'ayah':
                merged.append(ExtractedText(
                    text=ai_region.text or '',
                    bbox=ai_region.bbox,
                    page=ai_analysis.page_number,
                    source='ai_only',
                    confidence=ai_region.confidence,
                    metadata={
                        'ai_text_type': ai_region.text_type,
                        'ai_font_type': ai_region.font_type
                    }
                ))
        
        return merged
    
    def _calculate_bbox_overlap(self, bbox1: List[float], 
                               bbox2: List[float]) -> float:
        """Ø­Ø³Ø§Ø¨ Ù†Ø³Ø¨Ø© Ø§Ù„ØªØ¯Ø§Ø®Ù„ Ø¨ÙŠÙ† Ù…Ø±Ø¨Ø¹ÙŠÙ† Ù…Ø­ÙŠØ·ÙŠÙ†"""
        x1_min, y1_min, x1_max, y1_max = bbox1
        x2_min, y2_min, x2_max, y2_max = bbox2
        
        # Ø­Ø³Ø§Ø¨ Ø§Ù„ØªÙ‚Ø§Ø·Ø¹
        x_overlap = max(0, min(x1_max, x2_max) - max(x1_min, x2_min))
        y_overlap = max(0, min(y1_max, y2_max) - max(y1_min, y2_min))
        
        intersection = x_overlap * y_overlap
        
        # Ø­Ø³Ø§Ø¨ Ø§Ù„Ø§ØªØ­Ø§Ø¯
        area1 = (x1_max - x1_min) * (y1_max - y1_min)
        area2 = (x2_max - x2_min) * (y2_max - y2_min)
        union = area1 + area2 - intersection
        
        return intersection / union if union > 0 else 0.0
    
    def remove_unwanted_elements(self, page_num: int) -> Optional[ExtractedText]:
        """Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø¹Ù†Ø§ØµØ± ØºÙŠØ± Ø§Ù„Ù…Ø±ØºÙˆØ¨Ø© Ù…Ù† Ø§Ù„ØµÙØ­Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… AI"""
        if not self.ai_analyzer:
            return None
            
        try:
            # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…Ø³Ø§Ø± Ø§Ù„ØµÙˆØ±Ø©
            svg_path = Path(self.config['svg_folder']) / f"page_{page_num:03d}.svg"
            
            if not svg_path.exists():
                return None
            
            # ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØµÙØ­Ø©
            analysis = self.ai_analyzer.analyze_page(
                str(svg_path),
                ['text_regions', 'layout']
            )
            
            # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø¹Ù†Ø§ØµØ± Ø§Ù„Ù…Ø­Ø¯Ø¯Ø©
            elements_to_remove = self.config.get('remove_elements', [
                'page_number', 'decoration', 'footnote'
            ])
            
            # Ø¥Ù†Ø´Ø§Ø¡ ØµÙˆØ±Ø© Ù†Ø¸ÙŠÙØ©
            clean_image = self.ai_analyzer.remove_page_elements(
                str(svg_path),
                elements_to_remove
            )
            
            # Ø­ÙØ¸ Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ù†Ø¸ÙŠÙØ©
            clean_path = Path(self.config.get('clean_pages_dir', 'clean_pages'))
            clean_path.mkdir(exist_ok=True)
            
            output_file = clean_path / f"page_{page_num:03d}_clean.png"
            clean_image.save(output_file)
            
            self.logger.info(f"âœ… ØªÙ… ØªÙ†Ø¸ÙŠÙ Ø§Ù„ØµÙØ­Ø© {page_num}")
            self.stats['cleaned_pages'] += 1
            
            return output_file
            
        except Exception as e:
            self.logger.error(f"Ø®Ø·Ø£ ÙÙŠ ØªÙ†Ø¸ÙŠÙ Ø§Ù„ØµÙØ­Ø© {page_num}: {e}")
            return None

    def extract_from_svg(self, svg_path: str) -> List[ExtractedText]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†ØµÙˆØµ Ù…Ù† Ù…Ù„Ù SVG"""
        try:
            tree = ET.parse(svg_path)
            root = tree.getroot()
            texts = []
            # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø¬Ù…ÙŠØ¹ Ø¹Ù†Ø§ØµØ± Ø§Ù„Ù†Øµ
            for elem in root.iter():
                # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¹Ù†Ø§ØµØ± use Ù…Ø¹ data-text
                if elem.tag.endswith('use') and 'data-text' in elem.attrib:
                    text_content = elem.get('data-text', '')
                    transform = elem.get('transform', '')
                    if text_content.strip():
                        coords = self._extract_svg_coordinates(transform, elem)
                        if coords:
                            text_content = self._decode_html_entities(text_content)
                            texts.append(ExtractedText(
                                text=text_content,
                                bbox=coords,
                                page=self._get_page_from_filename(svg_path),
                                source='svg',
                                metadata={
                                    'transform': transform,
                                    'element_id': elem.get('id', ''),
                                    'href': elem.get('{http://www.w3.org/1999/xlink}href', '')
                                }
                            ))
                # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¹Ù†Ø§ØµØ± text
                elif elem.tag.endswith('text'):
                    text_content = self._extract_text_element_content(elem)
                    if text_content.strip():
                        x = float(elem.get('x', 0))
                        y = float(elem.get('y', 0))
                        bbox = self._estimate_text_bbox(text_content, x, y)
                        texts.append(ExtractedText(
                            text=text_content,
                            bbox=bbox,
                            page=self._get_page_from_filename(svg_path),
                            source='svg',
                            metadata={
                                'element_type': 'text',
                                'font_size': elem.get('font-size', ''),
                                'font_family': elem.get('font-family', '')
                            }
                        ))
            processed_texts = self._process_svg_texts(texts)
            self.logger.info(f"ğŸ“„ SVG: Ø§Ø³ØªØ®Ø±Ø¬ {len(processed_texts)} Ù†Øµ Ù…Ù† {svg_path}")
            return processed_texts
        except Exception as e:
            self.logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© SVG {svg_path}: {e}")
            return []
            
    def _extract_svg_coordinates(self, transform: str, elem: ET.Element) -> Optional[List[float]]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¥Ø­Ø¯Ø§Ø«ÙŠØ§Øª Ù…Ù† SVG transform"""
        try:
            # Ù…Ø¹Ø§Ù„Ø¬Ø© matrix transform
            matrix_match = re.search(
                r'matrix\(([^,]+),([^,]+),([^,]+),([^,]+),([^,]+),([^)]+)\)',
                transform
            )
            
            if matrix_match:
                # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù‚ÙŠÙ… Ø§Ù„Ù…ØµÙÙˆÙØ©
                a, b, c, d, e, f = map(float, matrix_match.groups())
                x, y = e, f
            else:
                # Ù…Ø¹Ø§Ù„Ø¬Ø© translate
                translate_match = re.search(
                    r'translate\(([^,]+)(?:,([^)]+))?\)',
                    transform
                )
                
                if translate_match:
                    x = float(translate_match.group(1))
                    y = float(translate_match.group(2) or 0)
                else:
                    # Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ x,y Ù…Ù† Ø§Ù„Ø®ØµØ§Ø¦Øµ
                    x = float(elem.get('x', 0))
                    y = float(elem.get('y', 0))
                    
            # ØªÙ‚Ø¯ÙŠØ± Ø§Ù„Ø­Ø¯ÙˆØ¯
            width = 100  # ØªÙ‚Ø¯ÙŠØ± Ø§ÙØªØ±Ø§Ø¶ÙŠ
            height = 20  # ØªÙ‚Ø¯ÙŠØ± Ø§ÙØªØ±Ø§Ø¶ÙŠ
            
            return [x, y - height, x + width, y]
            
        except Exception as e:
            self.logger.debug(f"ÙØ´Ù„ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¥Ø­Ø¯Ø§Ø«ÙŠØ§Øª: {e}")
            return None
            
    def _decode_html_entities(self, text: str) -> str:
        """ÙÙƒ ØªØ´ÙÙŠØ± HTML entities"""
        # Ù…Ø¹Ø§Ù„Ø¬Ø© &#x...;
        text = re.sub(
            r'&#x([0-9a-fA-F]+);',
            lambda m: chr(int(m.group(1), 16)),
            text
        )
        
        # Ù…Ø¹Ø§Ù„Ø¬Ø© &#...;
        text = re.sub(
            r'&#([0-9]+);',
            lambda m: chr(int(m.group(1))),
            text
        )
        
        return text
        
    def _extract_text_element_content(self, elem: ET.Element) -> str:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ø­ØªÙˆÙ‰ Ø¹Ù†ØµØ± text Ù…Ø¹ Ù…Ø¹Ø§Ù„Ø¬Ø© tspan"""
        text_parts = []
        
        # Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø¨Ø§Ø´Ø±
        if elem.text:
            text_parts.append(elem.text)
            
        # Ù…Ø¹Ø§Ù„Ø¬Ø© tspan
        for tspan in elem.findall('.//{http://www.w3.org/2000/svg}tspan'):
            if tspan.text:
                text_parts.append(tspan.text)
                
        # Ø§Ù„Ù†Øµ Ø¨Ø¹Ø¯ Ø§Ù„Ø¹Ù†Ø§ØµØ±
        if elem.tail:
            text_parts.append(elem.tail)
            
        return ''.join(text_parts)
        
    def _estimate_text_bbox(self, text: str, x: float, y: float) -> List[float]:
        """ØªÙ‚Ø¯ÙŠØ± Ø­Ø¯ÙˆØ¯ Ø§Ù„Ù†Øµ"""
        # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø®Ø· Ø¥Ù† Ø£Ù…ÙƒÙ†
        if self.font_manager:
            rendering_info = self.font_manager.get_rendering_info(text)
            estimated_width = rendering_info['estimated_width'] * 0.1  # ØªØ­ÙˆÙŠÙ„ Ù…Ù† ÙˆØ­Ø¯Ø§Øª Ø§Ù„Ø®Ø·
        else:
            # ØªÙ‚Ø¯ÙŠØ± Ø¨Ø³ÙŠØ·
            estimated_width = len(text) * 10
            
        estimated_height = 20
        
        return [x, y - estimated_height, x + estimated_width, y]
        
    def _get_page_from_filename(self, filename: str) -> int:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø±Ù‚Ù… Ø§Ù„ØµÙØ­Ø© Ù…Ù† Ø§Ø³Ù… Ø§Ù„Ù…Ù„Ù"""
        match = re.search(r'page[_-]?(\d+)', filename, re.IGNORECASE)
        if match:
            return int(match.group(1))
            
        match = re.search(r'(\d+)', filename)
        if match:
            return int(match.group(1))
            
        return 1
        
    def _process_svg_texts(self, texts: List[ExtractedText]) -> List[ExtractedText]:
        """Ù…Ø¹Ø§Ù„Ø¬Ø© ÙˆØªØ¬Ù…ÙŠØ¹ Ø§Ù„Ù†ØµÙˆØµ Ù…Ù† SVG"""
        if not texts:
            return []
            
        # ØªØ±ØªÙŠØ¨ Ø­Ø³Ø¨ Ø§Ù„Ù…ÙˆØ¶Ø¹
        texts.sort(key=lambda t: (-t.bbox[1], t.bbox[0]))
        
        # ØªØ¬Ù…ÙŠØ¹ Ø§Ù„Ù†ØµÙˆØµ ÙÙŠ Ø£Ø³Ø·Ø±
        lines = []
        current_line = []
        current_y = None
        line_threshold = self.config.get('line_threshold', 10)
        
        for text in texts:
            y_center = (text.bbox[1] + text.bbox[3]) / 2
            
            if current_y is None:
                current_y = y_center
                current_line = [text]
            elif abs(y_center - current_y) <= line_threshold:
                # Ù†ÙØ³ Ø§Ù„Ø³Ø·Ø±
                current_line.append(text)
            else:
                # Ø³Ø·Ø± Ø¬Ø¯ÙŠØ¯
                if current_line:
                    lines.append(self._merge_line_texts(current_line))
                current_line = [text]
                current_y = y_center
                
        # Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ø³Ø·Ø± Ø§Ù„Ø£Ø®ÙŠØ±
        if current_line:
            lines.append(self._merge_line_texts(current_line))
            
        return lines
        
    def _merge_line_texts(self, line_texts: List[ExtractedText]) -> ExtractedText:
        """Ø¯Ù…Ø¬ Ù†ØµÙˆØµ Ø§Ù„Ø³Ø·Ø± Ø§Ù„ÙˆØ§Ø­Ø¯"""
        # ØªØ±ØªÙŠØ¨ Ù…Ù† Ø§Ù„ÙŠÙ…ÙŠÙ† Ù„Ù„ÙŠØ³Ø§Ø± (Ù„Ù„Ø¹Ø±Ø¨ÙŠØ©)
        line_texts.sort(key=lambda t: -t.bbox[0])
        
        # Ø¯Ù…Ø¬ Ø§Ù„Ù†ØµÙˆØµ
        merged_text = ''.join(t.text for t in line_texts)
        
        # Ø­Ø³Ø§Ø¨ Ø§Ù„Ø­Ø¯ÙˆØ¯ Ø§Ù„ÙƒÙ„ÙŠØ©
        min_x = min(t.bbox[0] for t in line_texts)
        min_y = min(t.bbox[1] for t in line_texts)
        max_x = max(t.bbox[2] for t in line_texts)
        max_y = max(t.bbox[3] for t in line_texts)
        
        return ExtractedText(
            text=merged_text,
            bbox=[min_x, min_y, max_x, max_y],
            page=line_texts[0].page,
            source='svg',
            confidence=min(t.confidence for t in line_texts),
            metadata={
                'merged_from': len(line_texts),
                'original_texts': [t.text for t in line_texts]
            }
        )
        
    def extract_from_pdf(self, pdf_path: str, page_num: int) -> List[ExtractedText]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†ØµÙˆØµ Ù…Ù† ØµÙØ­Ø© PDF"""
        try:
            doc = fitz.open(pdf_path)
            
            if page_num < 0 or page_num >= doc.page_count:
                self.logger.error(f"Ø±Ù‚Ù… Ø§Ù„ØµÙØ­Ø© {page_num} Ø®Ø§Ø±Ø¬ Ø§Ù„Ù†Ø·Ø§Ù‚")
                return []
                
            page = doc[page_num]
            texts = []
            
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†ØµÙˆØµ Ù…Ø¹ Ø§Ù„Ø¥Ø­Ø¯Ø§Ø«ÙŠØ§Øª
            text_dict = page.get_text("dict", flags=fitz.TEXT_PRESERVE_LIGATURES)
            
            for block in text_dict["blocks"]:
                if block["type"] == 0:  # Ù†Øµ
                    for line in block["lines"]:
                        line_text = ""
                        line_bbox = None
                        
                        for span in line["spans"]:
                            span_text = span["text"]
                            
                            # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù†Øµ
                            if span_text.strip():
                                line_text += span_text
                                
                                # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø­Ø¯ÙˆØ¯
                                span_bbox = span["bbox"]
                                if line_bbox is None:
                                    line_bbox = list(span_bbox)
                                else:
                                    line_bbox[0] = min(line_bbox[0], span_bbox[0])
                                    line_bbox[1] = min(line_bbox[1], span_bbox[1])
                                    line_bbox[2] = max(line_bbox[2], span_bbox[2])
                                    line_bbox[3] = max(line_bbox[3], span_bbox[3])
                                    
                        if line_text.strip() and line_bbox:
                            texts.append(ExtractedText(
                                text=line_text.strip(),
                                bbox=line_bbox,
                                page=page_num + 1,
                                source='pdf',
                                line_number=line["line_no"],
                                metadata={
                                    'font': span.get("font", ""),
                                    'size': span.get("size", 0),
                                    'flags': span.get("flags", 0),
                                    'color': span.get("color", 0)
                                }
                            ))
                            
            doc.close()
            
            self.logger.info(f"ğŸ“„ PDF: Ø§Ø³ØªØ®Ø±Ø¬ {len(texts)} Ù†Øµ Ù…Ù† Ø§Ù„ØµÙØ­Ø© {page_num + 1}")
            return texts
            
        except Exception as e:
            self.logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© PDF: {e}")
            return []
            
    def match_ayat_with_texts(self, ayat_df: pd.DataFrame, extracted_texts: List[ExtractedText],
                             page_num: int) -> List[MatchedAyah]:
        """Ù…Ø·Ø§Ø¨Ù‚Ø© Ø§Ù„Ø¢ÙŠØ§Øª Ù…Ø¹ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬Ø©"""
        # ÙÙ„ØªØ±Ø© Ø¢ÙŠØ§Øª Ø§Ù„ØµÙØ­Ø©
        page_ayat = ayat_df[ayat_df['page'] == page_num].copy()
        
        if page_ayat.empty:
            self.logger.warning(f"Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¢ÙŠØ§Øª ÙÙŠ Ø§Ù„ØµÙØ­Ø© {page_num}")
            return []
            
        matches = []
        used_texts = set()
        
        # Ù…Ø¹Ø§Ù„Ø¬Ø© ÙƒÙ„ Ø¢ÙŠØ©
        for idx, aya_row in page_ayat.iterrows():
            # Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø±Ø¬Ø¹ÙŠ
            ref_text = aya_row['aya_text']
            ref_clean = aya_row['text_clean']
            aya_number = int(aya_row['aya_no'])
            
            # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø£ÙØ¶Ù„ Ù…Ø·Ø§Ø¨Ù‚Ø©
            best_match = None
            best_score = 0.0
            best_text_idx = -1
            
            for text_idx, extracted in enumerate(extracted_texts):
                if text_idx in used_texts:
                    continue
                    
                # Ø­Ø³Ø§Ø¨ Ø§Ù„ØªØ´Ø§Ø¨Ù‡
                score = self._calculate_match_score(
                    ref_text, ref_clean, extracted.text, aya_number
                )
                
                if score > best_score and score >= self.config['min_similarity']:
                    best_score = score
                    best_match = extracted
                    best_text_idx = text_idx
                    
            # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚Ø©
            if best_match:
                used_texts.add(best_text_idx)
                
                match = MatchedAyah(
                    page=page_num,
                    sura_no=int(aya_row['sura_no']),
                    sura_name_ar=aya_row['sura_name_ar'],
                    sura_name_en=aya_row['sura_name_en'],
                    aya_no=aya_number,
                    text_uthmani=ref_text,
                    text_extracted=best_match.text,
                    text_clean=self.text_processor.clean_text(best_match.text),
                    similarity=best_score,
                    bbox=best_match.bbox,
                    source=best_match.source,
                    extraction_method=self._determine_extraction_method(best_match),
                    confidence=best_match.confidence * best_score,
                    metadata={
                        'line_number': best_match.line_number,
                        'text_analysis': self.text_processor.analyze_text(best_match.text).__dict__,
                        'match_details': self._get_match_details(ref_text, best_match.text)
                    }
                )
                
                matches.append(match)
                self.stats['matched'] += 1
                
                self.logger.debug(
                    f"âœ“ Ù…Ø·Ø§Ø¨Ù‚Ø© {aya_row['sura_no']}:{aya_number} "
                    f"(ØªØ´Ø§Ø¨Ù‡: {best_score:.3f}, Ù…ØµØ¯Ø±: {best_match.source})"
                )
            else:
                self.stats['unmatched'] += 1
                self.logger.warning(
                    f"âœ— Ù„Ù… ÙŠØªÙ… Ù…Ø·Ø§Ø¨Ù‚Ø© {aya_row['sura_no']}:{aya_number}"
                )
                
        return matches
        
    def _calculate_match_score(self, ref_text: str, ref_clean: str, 
                              extracted_text: str, aya_number: int) -> float:
        """Ø­Ø³Ø§Ø¨ Ø¯Ø±Ø¬Ø© Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚Ø© Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©"""
        # Ø§Ù„ØªØ´Ø§Ø¨Ù‡ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ
        basic_score = self.text_processor.text_similarity(
            ref_text, extracted_text, method='combined'
        )
        
        # Ù…ÙƒØ§ÙØ£Ø© Ø±Ù‚Ù… Ø§Ù„Ø¢ÙŠØ©
        verse_bonus = 0.0
        extracted_verses = self.text_processor.extract_verse_numbers(extracted_text)
        
        if extracted_verses:
            for verse in extracted_verses:
                if verse['value'] == aya_number:
                    verse_bonus = 0.15
                    break
                    
        # Ù…ÙƒØ§ÙØ£Ø© Ø§Ù„Ø±Ù…ÙˆØ² Ø§Ù„Ø¹Ø«Ù…Ø§Ù†ÙŠØ©
        symbols_bonus = 0.0
        if self.font_manager:
            ref_symbols = set()
            ext_symbols = set()
            
            for char in ref_text:
                if self.font_manager.is_uthmani_symbol(char):
                    ref_symbols.add(char)
                    
            for char in extracted_text:
                if self.font_manager.is_uthmani_symbol(char):
                    ext_symbols.add(char)
                    
            if ref_symbols and ext_symbols:
                common = len(ref_symbols.intersection(ext_symbols))
                total = len(ref_symbols.union(ext_symbols))
                symbols_bonus = (common / total) * 0.1 if total > 0 else 0.0
                
        # Ø§Ù„Ø¯Ø±Ø¬Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©
        final_score = min(basic_score + verse_bonus + symbols_bonus, 1.0)
        
        return final_score
        
    def _determine_extraction_method(self, extracted: ExtractedText) -> str:
        """ØªØ­Ø¯ÙŠØ¯ Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬"""
        if extracted.source == 'svg':
            if extracted.metadata.get('merged_from', 0) > 1:
                return 'svg_merged'
            else:
                return 'svg_direct'
        else:
            return 'pdf_text'
            
    def _get_match_details(self, ref_text: str, extracted_text: str) -> Dict[str, Any]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ ØªÙØ§ØµÙŠÙ„ Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚Ø©"""
        return {
            'ref_length': len(ref_text),
            'extracted_length': len(extracted_text),
            'length_ratio': len(extracted_text) / len(ref_text) if len(ref_text) > 0 else 0,
            'ref_words': len(ref_text.split()),
            'extracted_words': len(extracted_text.split()),
            'has_verse_number': bool(self.text_processor.extract_verse_numbers(extracted_text)),
            'ref_symbols': len([c for c in ref_text if self.font_manager and self.font_manager.is_uthmani_symbol(c)]),
            'extracted_symbols': len([c for c in extracted_text if self.font_manager and self.font_manager.is_uthmani_symbol(c)])
        }
        
    def process_page(self, page_num: int) -> List[MatchedAyah]:
        """Ù…Ø¹Ø§Ù„Ø¬Ø© ØµÙØ­Ø© ÙˆØ§Ø­Ø¯Ø© Ù…Ø¹ Ø¯Ø¹Ù… Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ"""
        self.logger.info(f"\n{'='*50}")
        self.logger.info(f"Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØµÙØ­Ø© {page_num}")
        self.logger.info(f"{'='*50}")
        
        # ØªÙ†Ø¸ÙŠÙ Ø§Ù„ØµÙØ­Ø© Ø¨Ù€ AI Ø¥Ù† ÙƒØ§Ù† Ù…Ø·Ù„ÙˆØ¨Ø§Ù‹
        if self.ai_analyzer and self.config.get('ai_clean_pages', False):
            clean_result = self.remove_unwanted_elements(page_num)
            if clean_result:
                self.logger.info(f"ØªÙ… ØªÙ†Ø¸ÙŠÙ Ø§Ù„ØµÙØ­Ø© Ø¨Ù†Ø¬Ø§Ø­")
        
        extracted_texts = []
        
        # Ù…Ø­Ø§ÙˆÙ„Ø© SVG Ø£ÙˆÙ„Ø§Ù‹ Ù…Ø¹ ØªØ­Ø³ÙŠÙ† AI
        if self.config.get('prefer_svg', True) and self.config.get('svg_folder'):
            svg_path = Path(self.config['svg_folder']) / f"page_{page_num:03d}.svg"
            
            if svg_path.exists():
                if self.ai_analyzer and self.config.get('use_ai_for_svg', True):
                    svg_texts = self.extract_from_svg_with_ai(str(svg_path))
                else:
                    svg_texts = self.extract_from_svg(str(svg_path))
                    
                extracted_texts.extend(svg_texts)
                self.stats['svg_pages'] += 1
                
        # Ø¥Ø°Ø§ Ù„Ù… Ù†Ø­ØµÙ„ Ø¹Ù„Ù‰ Ù†ØµÙˆØµ ÙƒØ§ÙÙŠØ©ØŒ Ù†Ø¬Ø±Ø¨ PDF
        if len(extracted_texts) < 5 and self.config.get('pdf_path'):
            pdf_texts = self.extract_from_pdf(
                self.config['pdf_path'], 
                page_num - 1  # PDF zero-indexed
            )
            extracted_texts.extend(pdf_texts)
            self.stats['pdf_pages'] += 1
            
        # ØªØ­Ù„ÙŠÙ„ AI Ø¥Ø¶Ø§ÙÙŠ Ø¥Ø°Ø§ Ù„Ù… Ù†Ø­ØµÙ„ Ø¹Ù„Ù‰ Ù†ØªØ§Ø¦Ø¬ Ø¬ÙŠØ¯Ø©
        if self.ai_analyzer and len(extracted_texts) < 3:
            self.logger.info("Ù†ØªØ§Ø¦Ø¬ Ù‚Ù„ÙŠÙ„Ø©ØŒ Ù…Ø­Ø§ÙˆÙ„Ø© ØªØ­Ù„ÙŠÙ„ AI Ù…Ø¨Ø§Ø´Ø±...")
            
            try:
                # ØªØ­Ù„ÙŠÙ„ Ù…Ø¨Ø§Ø´Ø± Ù„Ù„ØµÙØ­Ø©
                page_path = None
                if self.config.get('svg_folder'):
                    svg_path = Path(self.config['svg_folder']) / f"page_{page_num:03d}.svg"
                    if svg_path.exists():
                        page_path = str(svg_path)
                        
                if page_path:
                    ai_analysis = self.ai_analyzer.analyze_page(
                        page_path,
                        ['text_regions', 'ayah_detection']
                    )
                    
                    # ØªØ­ÙˆÙŠÙ„ Ù…Ù†Ø§Ø·Ù‚ AI Ø¥Ù„Ù‰ ExtractedText
                    for region in ai_analysis.text_regions:
                        if region.text_type == 'ayah':
                            extracted_texts.append(ExtractedText(
                                text=region.text or '',
                                bbox=region.bbox,
                                page=page_num,
                                source='ai_direct',
                                confidence=region.confidence,
                                metadata={
                                    'ai_font_type': region.font_type
                                }
                            ))
                            
                    self.stats['ai_direct_pages'] += 1
                    
            except Exception as e:
                self.logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø¨Ø§Ø´Ø± Ø¨Ù€ AI: {e}")
            
        if not extracted_texts:
            self.logger.warning(f"Ù„Ù… ÙŠØªÙ… Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø£ÙŠ Ù†ØµÙˆØµ Ù…Ù† Ø§Ù„ØµÙØ­Ø© {page_num}")
            return []
            
        # Ù…Ø·Ø§Ø¨Ù‚Ø© Ø§Ù„Ø¢ÙŠØ§Øª
        matches = self.match_ayat_with_texts(
            self.hafs_data, extracted_texts, page_num
        )
        
        self.stats['total_pages'] += 1
        self.logger.info(
            f"Ø§Ù„ØµÙØ­Ø© {page_num}: {len(matches)} Ø¢ÙŠØ© Ù…Ø·Ø§Ø¨Ù‚Ø© Ù…Ù† "
            f"{len(extracted_texts)} Ù†Øµ Ù…Ø³ØªØ®Ø±Ø¬"
        )
        
        return matches
        
    def process_pages(self, start_page: int, end_page: int, 
                     parallel: bool = True) -> List[MatchedAyah]:
        """Ù…Ø¹Ø§Ù„Ø¬Ø© Ù†Ø·Ø§Ù‚ Ù…Ù† Ø§Ù„ØµÙØ­Ø§Øª"""
        all_matches = []
        pages = list(range(start_page, end_page + 1))
        
        if parallel and len(pages) > 1:
            # Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…ØªÙˆØ§Ø²ÙŠØ©
            max_workers = min(self.config.get('max_workers', 4), len(pages))
            
            with ThreadPoolExecutor(max_workers=max_workers) as executor:
                # Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„Ù…Ù‡Ø§Ù…
                futures = {
                    executor.submit(self.process_page, page): page 
                    for page in pages
                }
                
                # Ø¬Ù…Ø¹ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ù…Ø¹ Ø´Ø±ÙŠØ· Ø§Ù„ØªÙ‚Ø¯Ù…
                with tqdm(total=len(pages), desc="Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØµÙØ­Ø§Øª") as pbar:
                    for future in as_completed(futures):
                        page = futures[future]
                        try:
                            matches = future.result()
                            all_matches.extend(matches)
                        except Exception as e:
                            self.logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØµÙØ­Ø© {page}: {e}")
                        finally:
                            pbar.update(1)
        else:
            # Ù…Ø¹Ø§Ù„Ø¬Ø© ØªØ³Ù„Ø³Ù„ÙŠØ©
            for page in tqdm(pages, desc="Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØµÙØ­Ø§Øª"):
                try:
                    matches = self.process_page(page)
                    all_matches.extend(matches)
                except Exception as e:
                    self.logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØµÙØ­Ø© {page}: {e}")
                    
        return all_matches
        
    def save_results(self, matches: List[MatchedAyah], output_file: str):
        """Ø­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ÙÙŠ Ù…Ù„Ù JSON"""
        # ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ Ù‚Ø§Ù…ÙˆØ³
        results = []
        for match in matches:
            result = {
                'page': match.page,
                'sura_no': match.sura_no,
                'sura_name_ar': match.sura_name_ar,
                'sura_name_en': match.sura_name_en,
                'aya_no': match.aya_no,
                'text_uthmani': match.text_uthmani,
                'text_extracted': match.text_extracted,
                'text_clean': match.text_clean,
                'similarity': round(match.similarity, 4),
                'bbox': match.bbox,
                'source': match.source,
                'extraction_method': match.extraction_method,
                'confidence': round(match.confidence, 4),
                'metadata': match.metadata
            }
            results.append(result)
            
        # Ø¥Ø¶Ø§ÙØ© Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø¹Ø§Ù…Ø©
        output_data = {
            'metadata': {
                'generated_at': datetime.now().isoformat(),
                'total_ayat': len(results),
                'pages_processed': self.stats['total_pages'],
                'config': self.config,
                'statistics': dict(self.stats),
                'script_version': '3.0'
            },
            'results': results
        }
        
        # Ø­ÙØ¸ Ø§Ù„Ù…Ù„Ù
        output_path = Path(output_file)
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(output_data, f, ensure_ascii=False, indent=2)
            
        self.logger.info(f"âœ… ØªÙ… Ø­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ÙÙŠ: {output_path}")
        
    def generate_report(self, matches: List[MatchedAyah]) -> Dict[str, Any]:
        """ØªÙˆÙ„ÙŠØ¯ ØªÙ‚Ø±ÙŠØ± Ù…ÙØµÙ„"""
        if not matches:
            return {'error': 'Ù„Ø§ ØªÙˆØ¬Ø¯ Ù†ØªØ§Ø¦Ø¬'}
            
        # Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø¹Ø§Ù…Ø©
        total = len(matches)
        similarities = [m.similarity for m in matches]
        confidences = [m.confidence for m in matches]
        
        # ØªØµÙ†ÙŠÙ Ø­Ø³Ø¨ Ø§Ù„Ø¬ÙˆØ¯Ø©
        excellent = sum(1 for s in similarities if s >= 0.8)
        good = sum(1 for s in similarities if 0.6 <= s < 0.8)
        fair = sum(1 for s in similarities if 0.4 <= s < 0.6)
        poor = sum(1 for s in similarities if s < 0.4)
        
        # ØªØµÙ†ÙŠÙ Ø­Ø³Ø¨ Ø§Ù„Ù…ØµØ¯Ø±
        by_source = defaultdict(int)
        by_method = defaultdict(int)
        
        for match in matches:
            by_source[match.source] += 1
            by_method[match.extraction_method] += 1
            
        # ØµÙØ­Ø§Øª Ø¨Ù‡Ø§ Ù…Ø´Ø§ÙƒÙ„
        pages_with_issues = []
        page_stats = defaultdict(lambda: {'total': 0, 'poor': 0})
        
        for match in matches:
            page_stats[match.page]['total'] += 1
            if match.similarity < 0.4:
                page_stats[match.page]['poor'] += 1
                
        for page, stats in page_stats.items():
            if stats['poor'] / stats['total'] > 0.2:
                pages_with_issues.append({
                    'page': page,
                    'total': stats['total'],
                    'poor': stats['poor'],
                    'ratio': stats['poor'] / stats['total']
                })
                
        report = {
            'summary': {
                'total_ayat': total,
                'pages_processed': self.stats['total_pages'],
                'avg_similarity': np.mean(similarities),
                'std_similarity': np.std(similarities),
                'min_similarity': np.min(similarities),
                'max_similarity': np.max(similarities),
                'avg_confidence': np.mean(confidences),
            },
            'quality_distribution': {
                'excellent': {'count': excellent, 'percentage': excellent/total*100},
                'good': {'count': good, 'percentage': good/total*100},
                'fair': {'count': fair, 'percentage': fair/total*100},
                'poor': {'count': poor, 'percentage': poor/total*100}
            },
            'by_source': dict(by_source),
            'by_method': dict(by_method),
            'pages_with_issues': sorted(pages_with_issues, key=lambda x: x['ratio'], reverse=True),
            'recommendations': self._generate_recommendations(matches, similarities)
        }
        
        return report
        
    def _generate_recommendations(self, matches: List[MatchedAyah], 
                                 similarities: List[float]) -> List[str]:
        """ØªÙˆÙ„ÙŠØ¯ Ø§Ù„ØªÙˆØµÙŠØ§Øª"""
        recommendations = []
        
        avg_sim = np.mean(similarities)
        poor_ratio = sum(1 for s in similarities if s < 0.4) / len(similarities)
        
        if avg_sim < 0.6:
            recommendations.append("Ù…ØªÙˆØ³Ø· Ø§Ù„ØªØ´Ø§Ø¨Ù‡ Ù…Ù†Ø®ÙØ¶ØŒ ÙŠÙÙ†ØµØ­ Ø¨Ù…Ø±Ø§Ø¬Ø¹Ø© Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©")
            
        if poor_ratio > 0.2:
            recommendations.append(f"{poor_ratio*100:.1f}% Ù…Ù† Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚Ø§Øª Ø¶Ø¹ÙŠÙØ©ØŒ Ù‚Ø¯ ØªØ­ØªØ§Ø¬ Ù„ØªØ­Ø³ÙŠÙ† Ø¬ÙˆØ¯Ø© Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…ØµØ¯Ø±")
            
        svg_ratio = self.stats.get('svg_pages', 0) / self.stats.get('total_pages', 1)
        if svg_ratio < 0.5:
            recommendations.append("Ø§Ù„Ø§Ø¹ØªÙ…Ø§Ø¯ Ø¹Ù„Ù‰ PDF Ø£ÙƒØ«Ø± Ù…Ù† SVGØŒ Ø§Ø³ØªØ®Ø¯Ø§Ù… SVG Ù‚Ø¯ ÙŠØ­Ø³Ù† Ø§Ù„Ø¯Ù‚Ø©")
            
        if self.stats.get('unmatched', 0) > self.stats.get('matched', 1) * 0.1:
            recommendations.append("Ù†Ø³Ø¨Ø© Ø¹Ø§Ù„ÙŠØ© Ù…Ù† Ø§Ù„Ø¢ÙŠØ§Øª ØºÙŠØ± Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚Ø©ØŒ ØªØ­Ù‚Ù‚ Ù…Ù† Ø¬ÙˆØ¯Ø© Ø§Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬")
            
        return recommendations
        
    def print_report(self, report: Dict[str, Any]):
        """Ø·Ø¨Ø§Ø¹Ø© Ø§Ù„ØªÙ‚Ø±ÙŠØ±"""
        print("\n" + "="*60)
        print("ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ")
        print("="*60)
        
        summary = report['summary']
        print(f"\nØ§Ù„Ù…Ù„Ø®Øµ:")
        print(f"  Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ø¢ÙŠØ§Øª: {summary['total_ayat']}")
        print(f"  Ø§Ù„ØµÙØ­Ø§Øª Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©: {summary['pages_processed']}")
        print(f"  Ù…ØªÙˆØ³Ø· Ø§Ù„ØªØ´Ø§Ø¨Ù‡: {summary['avg_similarity']:.3f} Â± {summary['std_similarity']:.3f}")
        print(f"  Ù†Ø·Ø§Ù‚ Ø§Ù„ØªØ´Ø§Ø¨Ù‡: {summary['min_similarity']:.3f} - {summary['max_similarity']:.3f}")
        print(f"  Ù…ØªÙˆØ³Ø· Ø§Ù„Ø«Ù‚Ø©: {summary['avg_confidence']:.3f}")
        
        print(f"\nØªÙˆØ²ÙŠØ¹ Ø§Ù„Ø¬ÙˆØ¯Ø©:")
        for level, data in report['quality_distribution'].items():
            print(f"  {level}: {data['count']} ({data['percentage']:.1f}%)")
            
        print(f"\nØ­Ø³Ø¨ Ø§Ù„Ù…ØµØ¯Ø±:")
        for source, count in report['by_source'].items():
            print(f"  {source}: {count}")
            
        print(f"\nØ­Ø³Ø¨ Ø§Ù„Ø·Ø±ÙŠÙ‚Ø©:")
        for method, count in report['by_method'].items():
            print(f"  {method}: {count}")
            
        if report['pages_with_issues']:
            print(f"\nØµÙØ­Ø§Øª Ø¨Ù‡Ø§ Ù…Ø´Ø§ÙƒÙ„:")
            for page_issue in report['pages_with_issues'][:5]:
                print(f"  Ø§Ù„ØµÙØ­Ø© {page_issue['page']}: "
                      f"{page_issue['poor']}/{page_issue['total']} "
                      f"({page_issue['ratio']*100:.1f}% Ø¶Ø¹ÙŠÙ)")
                      
        if report['recommendations']:
            print(f"\nØ§Ù„ØªÙˆØµÙŠØ§Øª:")
            for i, rec in enumerate(report['recommendations'], 1):
                print(f"  {i}. {rec}")
                
def main():
    """Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©"""
    # Ù…Ø¹Ø§Ù„Ø¬ Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª
    parser = argparse.ArgumentParser(
        description='Ù…Ø³ØªØ®Ø±Ø¬ Ø§Ù„Ø¢ÙŠØ§Øª Ø§Ù„Ù…ØªØ·ÙˆØ± Ù…Ù† Ù…Ù„ÙØ§Øª SVG Ùˆ PDF Ù…Ø¹ Ø¯Ø¹Ù… Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ',
        formatter_class=argparse.RawDescriptionHelpFormatter
    )
    
    parser.add_argument('--pdf-path', required=True,
                       help='Ù…Ø³Ø§Ø± Ù…Ù„Ù PDF')
    parser.add_argument('--svg-folder', required=True,
                       help='Ù…Ø¬Ù„Ø¯ Ù…Ù„ÙØ§Øª SVG')
    parser.add_argument('--csv-path', required=True,
                       help='Ù…Ø³Ø§Ø± Ù…Ù„Ù Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØµØ­Ù CSV')
    parser.add_argument('--font-file', required=True,
                       help='Ù…Ø³Ø§Ø± Ù…Ù„Ù Ø§Ù„Ø®Ø· TTF')
    parser.add_argument('--output', '-o', default='reports/ayat_coordinates.json',
                       help='Ù…Ù„Ù Ø§Ù„Ø¥Ø®Ø±Ø§Ø¬ JSON')
    parser.add_argument('--pages', nargs=2, type=int, metavar=('START', 'END'),
                       default=[1, 604], help='Ù†Ø·Ø§Ù‚ Ø§Ù„ØµÙØ­Ø§Øª')
    parser.add_argument('--min-similarity', type=float, default=0.3,
                       help='Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ø¯Ù†Ù‰ Ù„Ù„ØªØ´Ø§Ø¨Ù‡')
    parser.add_argument('--line-threshold', type=int, default=10,
                       help='Ø¹ØªØ¨Ø© ØªØ¬Ù…ÙŠØ¹ Ø§Ù„Ø£Ø³Ø·Ø±')
    parser.add_argument('--prefer-pdf', action='store_true',
                       help='ØªÙØ¶ÙŠÙ„ PDF Ø¹Ù„Ù‰ SVG')
    parser.add_argument('--parallel', action='store_true', default=True,
                       help='Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…ØªÙˆØ§Ø²ÙŠØ©')
    parser.add_argument('--max-workers', type=int, default=4,
                       help='Ø¹Ø¯Ø¯ Ø§Ù„Ø¹Ù…Ù„ÙŠØ§Øª Ø§Ù„Ù…ØªÙˆØ§Ø²ÙŠØ©')
    parser.add_argument('--cache-dir', default='font_cache',
                       help='Ù…Ø¬Ù„Ø¯ Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª')
    parser.add_argument('--log-dir', default='logs',
                       help='Ù…Ø¬Ù„Ø¯ Ø§Ù„Ø³Ø¬Ù„Ø§Øª')
    parser.add_argument('--verbose', '-v', action='store_true',
                       help='Ø¹Ø±Ø¶ ØªÙØ§ØµÙŠÙ„ Ø£ÙƒØ«Ø±')
    parser.add_argument('--report', action='store_true', default=True,
                       help='Ø¹Ø±Ø¶ Ø§Ù„ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ')
    
    # Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ
    ai_group = parser.add_argument_group('Ø®ÙŠØ§Ø±Ø§Øª Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ')
    ai_group.add_argument('--use-ai', action='store_true',
                         help='ØªÙØ¹ÙŠÙ„ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ')
    ai_group.add_argument('--groq-api-key', 
                         default=os.getenv('GROQ_API_KEY'),
                         help='Ù…ÙØªØ§Ø­ Groq API (Ø£Ùˆ Ù…Ù† Ù…ØªØºÙŠØ± Ø§Ù„Ø¨ÙŠØ¦Ø© GROQ_API_KEY)')
    ai_group.add_argument('--ai-model', 
                         choices=['fast', 'accurate', 'arabic', 'balanced'],
                         default='balanced',
                         help='Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…')
    ai_group.add_argument('--ai-clean-pages', action='store_true',
                         help='ØªÙ†Ø¸ÙŠÙ Ø§Ù„ØµÙØ­Ø§Øª Ø¨Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ')
    ai_group.add_argument('--remove-elements', nargs='+',
                         default=['page_number', 'decoration'],
                         help='Ø§Ù„Ø¹Ù†Ø§ØµØ± Ø§Ù„Ù…Ø±Ø§Ø¯ Ø¥Ø²Ø§Ù„ØªÙ‡Ø§')
    ai_group.add_argument('--use-ai-for-svg', action='store_true', default=True,
                         help='Ø§Ø³ØªØ®Ø¯Ø§Ù… AI Ù„ØªØ­Ø³ÙŠÙ† Ø§Ø³ØªØ®Ø±Ø§Ø¬ SVG')
    ai_group.add_argument('--ai-cache-dir', default='ai_cache',
                         help='Ù…Ø¬Ù„Ø¯ ØªØ®Ø²ÙŠÙ† AI Ø§Ù„Ù…Ø¤Ù‚Øª')
    ai_group.add_argument('--clean-pages-dir', default='clean_pages',
                         help='Ù…Ø¬Ù„Ø¯ Ø§Ù„ØµÙØ­Ø§Øª Ø§Ù„Ù†Ø¸ÙŠÙØ©')
    
    args = parser.parse_args()
    
    # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ù…ÙØªØ§Ø­ API Ø¥Ø°Ø§ ÙƒØ§Ù† AI Ù…ÙØ¹Ù„Ø§Ù‹
    if args.use_ai and not args.groq_api_key:
        print("âŒ Ø®Ø·Ø£: --use-ai ÙŠØªØ·Ù„Ø¨ ØªÙˆÙÙŠØ± --groq-api-key Ø£Ùˆ ØªØ¹ÙŠÙŠÙ† GROQ_API_KEY")
        sys.exit(1)
    
    # Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªÙƒÙˆÙŠÙ†
    config = {
        'pdf_path': args.pdf_path,
        'svg_folder': args.svg_folder,
        'csv_path': args.csv_path,
        'font_file': args.font_file,
        'min_similarity': args.min_similarity,
        'line_threshold': args.line_threshold,
        'prefer_svg': not args.prefer_pdf,
        'max_workers': args.max_workers,
        'cache_dir': args.cache_dir,
        'log_dir': args.log_dir,
        'verbose': args.verbose,
        # Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª AI
        'use_ai': args.use_ai,
        'groq_api_key': args.groq_api_key,
        'ai_model': args.ai_model,
        'ai_clean_pages': args.ai_clean_pages,
        'remove_elements': args.remove_elements,
        'use_ai_for_svg': args.use_ai_for_svg,
        'ai_cache_dir': args.ai_cache_dir,
        'clean_pages_dir': args.clean_pages_dir
    }
    
    # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬
    extractor = AyatExtractor(config)
    
    # Ø¹Ø±Ø¶ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª AI Ø¥Ù† ÙƒØ§Ù†Øª Ù…ÙØ¹Ù„Ø©
    if config['use_ai']:
        print(f"\nğŸ¤– Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ù…ÙØ¹Ù„")
        print(f"   Ø§Ù„Ù†Ù…ÙˆØ°Ø¬: {config['ai_model']}")
        print(f"   ØªÙ†Ø¸ÙŠÙ Ø§Ù„ØµÙØ­Ø§Øª: {'Ù†Ø¹Ù…' if config['ai_clean_pages'] else 'Ù„Ø§'}")
        print(f"   Ø§Ù„Ø¹Ù†Ø§ØµØ± Ø§Ù„Ù…Ø­Ø°ÙˆÙØ©: {', '.join(config['remove_elements'])}")
    
    # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØµÙØ­Ø§Øª
    print(f"\nÙ…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØµÙØ­Ø§Øª {args.pages[0]} Ø¥Ù„Ù‰ {args.pages[1]}...")
    matches = extractor.process_pages(
        args.pages[0], 
        args.pages[1],
        parallel=args.parallel
    )
    
    # Ø­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
    if matches:
        extractor.save_results(matches, args.output)
        
        # Ø¹Ø±Ø¶ Ø§Ù„ØªÙ‚Ø±ÙŠØ±
        if args.report:
            report = extractor.generate_report(matches)
            
            # Ø¥Ø¶Ø§ÙØ© Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª AI Ù„Ù„ØªÙ‚Ø±ÙŠØ±
            if config['use_ai']:
                report['ai_statistics'] = {
                    'ai_enhanced_pages': extractor.stats.get('ai_enhanced_pages', 0),
                    'ai_direct_pages': extractor.stats.get('ai_direct_pages', 0),
                    'cleaned_pages': extractor.stats.get('cleaned_pages', 0)
                }
            
            extractor.print_report(report)
            
            # Ø­ÙØ¸ Ø§Ù„ØªÙ‚Ø±ÙŠØ±
            report_file = Path(args.output).with_suffix('.report.json')
            with open(report_file, 'w', encoding='utf-8') as f:
                json.dump(report, f, ensure_ascii=False, indent=2)
                
        print(f"\nâœ… Ø§ÙƒØªÙ…Ù„ Ø§Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¨Ù†Ø¬Ø§Ø­!")
        print(f"Ø§Ù„Ù†ØªØ§Ø¦Ø¬: {args.output}")
        
        # Ø¹Ø±Ø¶ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª AI
        if config['use_ai']:
            print(f"\nğŸ“Š Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ:")
            print(f"   ØµÙØ­Ø§Øª Ù…Ø­Ø³Ù†Ø© Ø¨Ù€ AI: {extractor.stats.get('ai_enhanced_pages', 0)}")
            print(f"   ØµÙØ­Ø§Øª Ø¨ØªØ­Ù„ÙŠÙ„ AI Ù…Ø¨Ø§Ø´Ø±: {extractor.stats.get('ai_direct_pages', 0)}")
            print(f"   ØµÙØ­Ø§Øª Ù…Ù†Ø¸ÙØ©: {extractor.stats.get('cleaned_pages', 0)}")
    else:
        print("\nâŒ Ù„Ù… ÙŠØªÙ… Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø£ÙŠ Ù†ØªØ§Ø¦Ø¬")
        
if __name__ == "__main__":
    import sys
    main()
