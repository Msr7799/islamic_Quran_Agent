#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Ù…Ø³ØªØ®Ø±Ø¬ Ø¥Ø­Ø¯Ø§Ø«ÙŠØ§Øª Ø§Ù„Ø¢ÙŠØ§Øª Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… OCR ÙˆÙ…Ø¹Ø§Ù„Ø¬Ø© SVG Ø§Ù„Ù…Ø­Ø³Ù†Ø©
ÙŠØ¯Ø¹Ù… Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†ØµÙˆØµ Ù…Ù† SVG Ù…Ø¹ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ø³Ø§ÙØ§Øª ÙˆØ§Ù„ØªØ·Ø¨ÙŠØ¹ Ø§Ù„Ù…ØªÙ‚Ø¯Ù…
"""

import json
import re
import os
import sys
from pathlib import Path
import xml.etree.ElementTree as ET
import pandas as pd
import unicodedata
from difflib import SequenceMatcher
import subprocess
from PIL import Image
import pytesseract
from fontTools.ttLib import TTFont


def normalize_unicode_text_advanced(text):
    """ØªØ·Ø¨ÙŠØ¹ Ø§Ù„Ù†Øµ Ø§Ù„Ø¹Ø±Ø¨ÙŠ Ø§Ù„Ù…Ø­Ø³Ù† Ù…Ø¹ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ø³Ø§ÙØ§Øª"""
    if not text:
        return ""

    # ØªØ·Ø¨ÙŠØ¹ Unicode Ù…ØªÙ‚Ø¯Ù…
    text = unicodedata.normalize('NFKC', text)

    # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ø³Ø§ÙØ§Øª Ø§Ù„Ù…Ø®ØªÙ„ÙØ©
    space_chars = {
        '\u0020': ' ',  # Ù…Ø³Ø§ÙØ© Ø¹Ø§Ø¯ÙŠØ©
        '\u00A0': ' ',  # Ù…Ø³Ø§ÙØ© ØºÙŠØ± Ù…Ù†Ù‚Ø³Ù…Ø©
        '\u2000': ' ',  # en quad
        '\u2001': ' ',  # em quad
        '\u2002': ' ',  # en space
        '\u2003': ' ',  # em space
        '\u2004': ' ',  # three-per-em space
        '\u2005': ' ',  # four-per-em space
        '\u2006': ' ',  # six-per-em space
        '\u2007': ' ',  # figure space
        '\u2008': ' ',  # punctuation space
        '\u2009': ' ',  # thin space
        '\u200A': ' ',  # hair space
        '\u202F': ' ',  # narrow no-break space
        '\u205F': ' ',  # medium mathematical space
        '\u3000': ' ',  # ideographic space
        '\t': ' ',      # tab
        '\n': ' ',      # newline
        '\r': ' ',      # carriage return
    }

    # Ø§Ø³ØªØ¨Ø¯Ø§Ù„ Ø¬Ù…ÙŠØ¹ Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ù…Ø³Ø§ÙØ§Øª Ø¨Ù…Ø³Ø§ÙØ© Ø¹Ø§Ø¯ÙŠØ©
    for space_char, replacement in space_chars.items():
        text = text.replace(space_char, replacement)

    # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù…Ø³Ø§ÙØ§Øª Ø§Ù„Ù…ØªØ¹Ø¯Ø¯Ø©
    text = re.sub(r'\s+', ' ', text)

    # Ø§Ø³ØªØ¨Ø¯Ø§Ù„ Ø§Ù„Ø£Ø­Ø±Ù Ø§Ù„Ù…ØªØ´Ø§Ø¨Ù‡Ø© ÙˆØ§Ù„Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„Ø¹Ø«Ù…Ø§Ù†ÙŠØ©
    replacements = {
        # Ø§Ù„Ø£Ù„Ù ÙˆØ£Ø´ÙƒØ§Ù„Ù‡Ø§ Ø§Ù„Ø¹Ø«Ù…Ø§Ù†ÙŠØ©
        'Ù±': 'Ø§',  # Ø£Ù„Ù ÙˆØµÙ„
        'Ø£': 'Ø§',  # Ù‡Ù…Ø²Ø© Ø¹Ù„Ù‰ Ø£Ù„Ù
        'Ø¥': 'Ø§',  # Ù‡Ù…Ø²Ø© ØªØ­Øª Ø£Ù„Ù
        'Ø¢': 'Ø§',  # Ù…Ø¯ Ø¹Ù„Ù‰ Ø£Ù„Ù
        'ïº': 'Ø§', 'ïº': 'Ø§',  # Ø£Ø´ÙƒØ§Ù„ Ø§Ù„Ø£Ù„Ù Ø§Ù„Ù…Ø®ØªÙ„ÙØ©

        # Ø§Ù„Ù‡Ù…Ø²Ø© ÙˆØ£Ø´ÙƒØ§Ù„Ù‡Ø§
        'Ø¤': 'Ùˆ',  # Ù‡Ù…Ø²Ø© Ø¹Ù„Ù‰ ÙˆØ§Ùˆ
        'Ø¦': 'ÙŠ',  # Ù‡Ù…Ø²Ø© Ø¹Ù„Ù‰ ÙŠØ§Ø¡
        'Ø¡': '',   # Ù‡Ù…Ø²Ø© Ù…Ù†ÙØ±Ø¯Ø© (ØªÙØ­Ø°Ù ÙÙŠ Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚Ø©)

        # Ø§Ù„ÙŠØ§Ø¡ ÙˆØ§Ù„Ø£Ù„Ù Ø§Ù„Ù…Ù‚ØµÙˆØ±Ø©
        'Ù‰': 'ÙŠ',  # Ø£Ù„Ù Ù…Ù‚ØµÙˆØ±Ø©
        'ï»¯': 'ÙŠ', 'ï»°': 'ÙŠ', 'ï»±': 'ÙŠ', 'ï»²': 'ÙŠ',  # Ø£Ø´ÙƒØ§Ù„ Ø§Ù„ÙŠØ§Ø¡

        # Ø§Ù„ØªØ§Ø¡ Ø§Ù„Ù…Ø±Ø¨ÙˆØ·Ø© ÙˆØ§Ù„Ù‡Ø§Ø¡
        'Ø©': 'Ù‡',  # ØªØ§Ø¡ Ù…Ø±Ø¨ÙˆØ·Ø©
        'ïº“': 'Ù‡', 'ïº”': 'Ù‡',  # Ø£Ø´ÙƒØ§Ù„ Ø§Ù„ØªØ§Ø¡ Ø§Ù„Ù…Ø±Ø¨ÙˆØ·Ø©

        # Ø§Ù„ÙˆØ§Ùˆ ÙˆØ£Ø´ÙƒØ§Ù„Ù‡Ø§
        'ï»­': 'Ùˆ', 'ï»®': 'Ùˆ',  # Ø£Ø´ÙƒØ§Ù„ Ø§Ù„ÙˆØ§Ùˆ
    }

    # ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ø§Ø³ØªØ¨Ø¯Ø§Ù„Ø§Øª
    for old_char, new_char in replacements.items():
        text = text.replace(old_char, new_char)

    return text.strip()

class AdvancedFontMatcher:
    """Ù…Ø·Ø§Ø¨Ù‚ Ù…ØªÙ‚Ø¯Ù… Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…Ù„Ù Ø§Ù„Ø®Ø· Ù…Ø¨Ø§Ø´Ø±Ø©"""

    def __init__(self, font_file="UthmanicHafs_v2-0 font/uthmanic_hafs_v20.ttf"):
        self.font = self.load_font_direct(font_file)
        self.character_map = self.build_character_map()
        self.normalization_rules = self.build_normalization_rules()

    def load_font_direct(self, font_file):
        """ØªØ­Ù…ÙŠÙ„ Ù…Ù„Ù Ø§Ù„Ø®Ø· Ù…Ø¨Ø§Ø´Ø±Ø©"""
        try:
            font = TTFont(font_file)
            print(f"âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø®Ø·: {font_file}")
            return font
        except Exception as e:
            print(f"Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø®Ø·: {e}")
            return None

    def build_character_map(self):
        """Ø¨Ù†Ø§Ø¡ Ø®Ø±ÙŠØ·Ø© Ø§Ù„Ø£Ø­Ø±Ù Ù…Ù† Ø§Ù„Ø®Ø· Ù…Ø¨Ø§Ø´Ø±Ø©"""
        if not self.font:
            return {}

        char_map = {}
        cmap = self.font.getBestCmap()

        for unicode_val, glyph_name in cmap.items():
            try:
                char = chr(unicode_val)
                char_map[char] = {
                    'unicode': f'U+{unicode_val:04X}',
                    'glyph_name': glyph_name,
                    'is_arabic': 0x0600 <= unicode_val <= 0x06FF,
                    'is_symbol': unicode_val > 0xFE00
                }
            except:
                continue

        print(f"âœ… ØªÙ… Ø¨Ù†Ø§Ø¡ Ø®Ø±ÙŠØ·Ø© {len(char_map)} Ø­Ø±Ù Ù…Ù† Ø§Ù„Ø®Ø·")
        return char_map

    def build_normalization_rules(self):
        """Ø¨Ù†Ø§Ø¡ Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„ØªØ·Ø¨ÙŠØ¹ Ù…Ù† Ø§Ù„Ø®Ø·"""
        return {
            # Ù‚ÙˆØ§Ø¹Ø¯ Ø£Ø³Ø§Ø³ÙŠØ© Ù„Ù„ØªØ·Ø¨ÙŠØ¹
            'Ù±': 'Ø§', 'Ø£': 'Ø§', 'Ø¥': 'Ø§', 'Ø¢': 'Ø§',
            'Ù‰': 'ÙŠ', 'Ø©': 'Ù‡', 'Ø¤': 'Ùˆ', 'Ø¦': 'ÙŠ'
        }
    
    def normalize_text_with_spaces(self, text):
        """ØªØ·Ø¨ÙŠØ¹ Ø§Ù„Ù†Øµ Ù…Ø¹ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ø³Ø§ÙØ§Øª Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø¬Ø¯ÙˆÙ„ Ø§Ù„Ø®Ø·"""
        if not text:
            return ""

        # Ø£ÙˆÙ„Ø§Ù‹: ØªØ·Ø¨ÙŠØ¹ Ø¹Ø§Ù… Ù„Ù„Ù†Øµ
        text = normalize_unicode_text_advanced(text)

        # Ø«Ø§Ù†ÙŠØ§Ù‹: ØªØ·Ø¨ÙŠÙ‚ Ù‚ÙˆØ§Ø¹Ø¯ Ø¬Ø¯ÙˆÙ„ Ø§Ù„Ø®Ø·
        normalized = ""
        for char in text:
            # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„ØªØ·Ø¨ÙŠØ¹ Ù…Ù† Ø¬Ø¯ÙˆÙ„ Ø§Ù„Ø®Ø·
            normalized_char = self.normalization_rules.get(char, char)
            normalized += normalized_char

        # Ø«Ø§Ù„Ø«Ø§Ù‹: Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ø³Ø§ÙØ§Øª Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©
        normalized = re.sub(r'\s+', ' ', normalized)  # ØªÙˆØ­ÙŠØ¯ Ø§Ù„Ù…Ø³Ø§ÙØ§Øª

        return normalized.strip()
    
    def remove_all_diacritics_precise(self, text):
        """Ø¥Ø²Ø§Ù„Ø© Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø±Ù…ÙˆØ² Ø§Ù„Ø¹Ø«Ù…Ø§Ù†ÙŠØ© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø®Ø·"""
        if not text:
            return ""

        cleaned = ""
        for char in text:
            char_info = self.character_map.get(char, {})
            # Ø¥Ø°Ø§ ÙƒØ§Ù† Ø±Ù…Ø² Ø¹Ø«Ù…Ø§Ù†ÙŠ Ø£Ùˆ ØªØ´ÙƒÙŠÙ„ØŒ ØªØ¬Ø§Ù‡Ù„Ù‡
            if char_info.get('is_symbol', False) or ord(char) in range(0x064B, 0x0670):
                continue
            # Ø¥Ø°Ø§ ÙƒØ§Ù† Ø­Ø±Ù Ø£Ø³Ø§Ø³ÙŠØŒ Ø£Ø¶ÙÙ‡
            cleaned += char

        return cleaned
    
    def get_character_variants(self, base_char):
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø¬Ù…ÙŠØ¹ Ø£Ø´ÙƒØ§Ù„ Ø§Ù„Ø­Ø±Ù Ù…Ù† Ø§Ù„Ø®Ø·"""
        variants = [base_char]
        # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø£Ø´ÙƒØ§Ù„ Ù…Ø®ØªÙ„ÙØ© Ù„Ù„Ø­Ø±Ù ÙÙŠ Ø§Ù„Ø®Ø·
        for char, info in self.character_map.items():
            if info.get('base_char') == base_char:
                variants.append(char)
        return variants
    
    def calculate_advanced_similarity(self, text1, text2):
        """Ø­Ø³Ø§Ø¨ Ø§Ù„ØªØ´Ø§Ø¨Ù‡ Ø§Ù„Ù…ØªÙ‚Ø¯Ù… Ù…Ø¹ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ø³Ø§ÙØ§Øª"""
        # ØªØ·Ø¨ÙŠØ¹ Ø§Ù„Ù†ØµÙˆØµ Ù…Ø¹ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ø³Ø§ÙØ§Øª
        norm1 = self.normalize_text_with_spaces(text1)
        norm2 = self.normalize_text_with_spaces(text2)

        # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø±Ù…ÙˆØ² Ø§Ù„Ø¹Ø«Ù…Ø§Ù†ÙŠØ©
        clean1 = self.remove_all_diacritics_precise(norm1)
        clean2 = self.remove_all_diacritics_precise(norm2)

        # Ø­Ø³Ø§Ø¨ Ø§Ù„ØªØ´Ø§Ø¨Ù‡
        if not clean1 or not clean2:
            return 0.0

        if clean1 == clean2:
            return 1.0

        # Ø§Ø³ØªØ®Ø¯Ø§Ù… SequenceMatcher Ù„Ù„Ù…Ø·Ø§Ø¨Ù‚Ø© Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©
        similarity = SequenceMatcher(None, clean1, clean2).ratio()

        # Ø¥Ø¶Ø§ÙØ© Ù…ÙƒØ§ÙØ£Ø© Ù„Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…Ø´ØªØ±ÙƒØ©
        words1 = set(clean1.split())
        words2 = set(clean2.split())

        if words1 and words2:
            word_similarity = len(words1 & words2) / len(words1 | words2)
            # Ø¯Ù…Ø¬ ØªØ´Ø§Ø¨Ù‡ Ø§Ù„Ø£Ø­Ø±Ù ÙˆØªØ´Ø§Ø¨Ù‡ Ø§Ù„ÙƒÙ„Ù…Ø§Øª
            similarity = (similarity * 0.7) + (word_similarity * 0.3)

        return similarity

class AdvancedAyatExtractor:
    """Ù…Ø³ØªØ®Ø±Ø¬ Ù…ØªÙ‚Ø¯Ù… Ù„Ø¥Ø­Ø¯Ø§Ø«ÙŠØ§Øª Ø§Ù„Ø¢ÙŠØ§Øª Ù…Ø¹ Ø¯Ø¹Ù… OCR ÙˆÙ…Ø¹Ø§Ù„Ø¬Ø© SVG"""

    def __init__(self, font_file="UthmanicHafs_v2-0 font/uthmanic_hafs_v20.ttf"):
        self.font_matcher = AdvancedFontMatcher(font_file)
        # Ø¥Ø¹Ø¯Ø§Ø¯ tesseract Ù„Ù„Ø¹Ø±Ø¨ÙŠØ©
        self.tesseract_config = '--oem 3 --psm 6 -l ara'
        
    def extract_text_from_svg(self, svg_file):
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†ØµÙˆØµ Ù…Ù† Ù…Ù„Ù SVG (ÙŠØ¯Ø¹Ù… Ø§Ù„Ù†ØµÙˆØµ ÙˆØ§Ù„Ù…Ø³Ø§Ø±Ø§Øª)"""
        try:
            tree = ET.parse(svg_file)
            root = tree.getroot()

            texts = []

            # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø¹Ù†Ø§ØµØ± Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø¨Ø§Ø´Ø±Ø©
            for elem in root.iter():
                if elem.tag.endswith('text') or elem.tag.endswith('tspan'):
                    if elem.text:
                        x = elem.get('x', '0')
                        y = elem.get('y', '0')

                        texts.append({
                            'text': elem.text.strip(),
                            'x': float(x) if x.replace('.', '').replace('-', '').isdigit() else 0,
                            'y': float(y) if y.replace('.', '').replace('-', '').isdigit() else 0,
                            'type': 'direct_text'
                        })

            # Ø¥Ø°Ø§ Ù„Ù… Ù†Ø¬Ø¯ Ù†ØµÙˆØµ Ù…Ø¨Ø§Ø´Ø±Ø©ØŒ Ù†Ø­Ø§ÙˆÙ„ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ù† Ø§Ù„Ù…Ø³Ø§Ø±Ø§Øª
            if not texts:
                print(f"  Ù„Ø§ ØªÙˆØ¬Ø¯ Ù†ØµÙˆØµ Ù…Ø¨Ø§Ø´Ø±Ø©ØŒ Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ù† Ø§Ù„Ù…Ø³Ø§Ø±Ø§Øª...")
                texts = self.extract_from_svg_paths(root)

            # Ø¥Ø°Ø§ ÙØ´Ù„ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…Ø³Ø§Ø±Ø§ØªØŒ Ù†Ø³ØªØ®Ø¯Ù… OCR
            if not texts or len(texts) < 10:  # Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ù‚Ù„ÙŠÙ„Ø©
                print(f"  Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ù…Ø³Ø§Ø±Ø§Øª Ù‚Ù„ÙŠÙ„Ø© ({len(texts)}), Ù…Ø­Ø§ÙˆÙ„Ø© OCR...")
                ocr_texts = self.extract_text_with_ocr(svg_file)
                if ocr_texts:
                    print(f"  ØªÙ… Ø§Ø³ØªØ®Ø±Ø§Ø¬ {len(ocr_texts)} Ù†Øµ Ø¨Ù€ OCR")
                    texts.extend(ocr_texts)

            return texts

        except Exception as e:
            print(f"Ø®Ø·Ø£ ÙÙŠ Ù‚Ø±Ø§Ø¡Ø© Ù…Ù„Ù SVG: {e}")
            return []

    def extract_from_svg_paths(self, root):
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†ØµÙˆØµ Ù…Ù† Ù…Ø³Ø§Ø±Ø§Øª SVG (ØªØ¬Ø±ÙŠØ¨ÙŠ)"""
        texts = []

        # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø¹Ù†Ø§ØµØ± use Ø§Ù„ØªÙŠ ØªØ´ÙŠØ± Ø¥Ù„Ù‰ Ø§Ù„Ø£Ø­Ø±Ù
        for elem in root.iter():
            if elem.tag.endswith('use'):
                href = elem.get('{http://www.w3.org/1999/xlink}href') or elem.get('href')
                if href and href.startswith('#font_'):
                    x = elem.get('x', '0')
                    y = elem.get('y', '0')

                    # Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ø¹Ø±Ù Ø§Ù„Ø­Ø±Ù Ù…Ù† href
                    font_id = href.replace('#', '')

                    texts.append({
                        'text': self.decode_font_id(font_id),
                        'x': float(x) if x.replace('.', '').replace('-', '').isdigit() else 0,
                        'y': float(y) if y.replace('.', '').replace('-', '').isdigit() else 0,
                        'type': 'font_path',
                        'font_id': font_id
                    })

        # Ø¥Ø°Ø§ Ù„Ù… Ù†Ø¬Ø¯ Ø¹Ù†Ø§ØµØ± useØŒ Ù†Ø­Ø§ÙˆÙ„ Ø·Ø±ÙŠÙ‚Ø© Ø£Ø®Ø±Ù‰
        if not texts:
            # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ù…Ø¬Ù…ÙˆØ¹Ø§Øª (groups) Ù‚Ø¯ ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ù†ØµÙˆØµ
            for elem in root.iter():
                if elem.tag.endswith('g'):  # group
                    # ÙØ­Øµ Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ù…Ø³Ø§Ø±Ø§Øª Ù†Øµ
                    paths_in_group = [child for child in elem if child.tag.endswith('path')]
                    if paths_in_group:
                        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¥Ø­Ø¯Ø§Ø«ÙŠØ§Øª Ù…Ù† transform Ø£Ùˆ Ù…Ù† Ø£ÙˆÙ„ Ù…Ø³Ø§Ø±
                        transform = elem.get('transform', '')
                        x, y = self.extract_coordinates_from_transform(transform)

                        texts.append({
                            'text': f"[Ù…Ø¬Ù…ÙˆØ¹Ø©_{len(texts)+1}]",  # Ù†Øµ ØªØ¬Ø±ÙŠØ¨ÙŠ
                            'x': x,
                            'y': y,
                            'type': 'group_paths',
                            'paths_count': len(paths_in_group)
                        })

        return texts

    def decode_font_id(self, font_id):
        """Ù…Ø­Ø§ÙˆÙ„Ø© ÙÙƒ ØªØ´ÙÙŠØ± Ù…Ø¹Ø±Ù Ø§Ù„Ø®Ø· Ø¥Ù„Ù‰ Ø­Ø±Ù"""
        # Ù‡Ø°Ù‡ Ø·Ø±ÙŠÙ‚Ø© ØªØ¬Ø±ÙŠØ¨ÙŠØ© - Ù‚Ø¯ Ù†Ø­ØªØ§Ø¬ Ù„ØªØ­Ø³ÙŠÙ†Ù‡Ø§
        if 'font_' in font_id:
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø±Ù‚Ù… Ù…Ù† Ù…Ø¹Ø±Ù Ø§Ù„Ø®Ø·
            parts = font_id.split('_')
            if len(parts) >= 3:
                try:
                    char_code = int(parts[2])
                    # Ù…Ø­Ø§ÙˆÙ„Ø© ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ Ø­Ø±Ù Unicode
                    if char_code < 1000:  # Ø£Ø±Ù‚Ø§Ù… Ø¨Ø³ÙŠØ·Ø©
                        return str(char_code)
                    else:
                        return chr(char_code) if char_code < 65536 else f"[{char_code}]"
                except:
                    return f"[{font_id}]"
        return f"[{font_id}]"

    def extract_coordinates_from_transform(self, transform):
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¥Ø­Ø¯Ø§Ø«ÙŠØ§Øª Ù…Ù† Ø®Ø§ØµÙŠØ© transform"""
        x, y = 0, 0
        if 'translate' in transform:
            import re
            match = re.search(r'translate\(([^)]+)\)', transform)
            if match:
                coords = match.group(1).split(',')
                try:
                    x = float(coords[0].strip())
                    y = float(coords[1].strip()) if len(coords) > 1 else 0
                except:
                    pass
        return x, y

    def svg_to_png_for_ocr(self, svg_file, png_file, dpi=300):
        """ØªØ­ÙˆÙŠÙ„ SVG Ø¥Ù„Ù‰ PNG Ù„Ù„Ù€ OCR"""
        try:
            # Ø§Ø³ØªØ®Ø¯Ø§Ù… inkscape Ù„ØªØ­ÙˆÙŠÙ„ SVG Ø¥Ù„Ù‰ PNG
            cmd = [
                'inkscape',
                '--export-type=png',
                f'--export-dpi={dpi}',
                f'--export-filename={png_file}',
                str(svg_file)
            ]

            result = subprocess.run(cmd, capture_output=True, text=True)
            return result.returncode == 0

        except Exception as e:
            print(f"Ø®Ø·Ø£ ÙÙŠ ØªØ­ÙˆÙŠÙ„ SVG Ø¥Ù„Ù‰ PNG: {e}")
            return False

    def extract_text_with_ocr(self, svg_file):
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†Øµ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… OCR ÙƒØ¨Ø¯ÙŠÙ„"""
        try:
            # Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù PNG Ù…Ø¤Ù‚Øª
            png_file = Path(str(svg_file).replace('.svg', '_temp.png'))

            # ØªØ­ÙˆÙŠÙ„ SVG Ø¥Ù„Ù‰ PNG
            if not self.svg_to_png_for_ocr(svg_file, png_file):
                return []

            # ÙØªØ­ Ø§Ù„ØµÙˆØ±Ø©
            image = Image.open(png_file)
            image = image.convert('RGB')

            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†Øµ Ù…Ø¹ Ø§Ù„Ø¥Ø­Ø¯Ø§Ø«ÙŠØ§Øª
            data = pytesseract.image_to_data(image, config=self.tesseract_config, output_type=pytesseract.Output.DICT)

            texts = []
            for i in range(len(data['text'])):
                text = data['text'][i].strip()
                if text and data['conf'][i] > 30:  # Ø«Ù‚Ø© Ø£ÙƒØ¨Ø± Ù…Ù† 30%
                    texts.append({
                        'text': text,
                        'x': data['left'][i],
                        'y': data['top'][i],
                        'width': data['width'][i],
                        'height': data['height'][i],
                        'confidence': data['conf'][i],
                        'type': 'ocr_text'
                    })

            # Ø­Ø°Ù Ø§Ù„Ù…Ù„Ù Ø§Ù„Ù…Ø¤Ù‚Øª
            if png_file.exists():
                png_file.unlink()

            return texts

        except Exception as e:
            print(f"Ø®Ø·Ø£ ÙÙŠ OCR: {e}")
            return []
    
    def find_verse_numbers_precise(self, texts):
        """Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø£Ø±Ù‚Ø§Ù… Ø§Ù„Ø¢ÙŠØ§Øª Ø¨Ø¯Ù‚Ø©"""
        verse_numbers = []
        
        # Ø§Ù„Ø£Ø±Ù‚Ø§Ù… Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© ÙˆØ§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ©
        arabic_digits = 'Ù Ù¡Ù¢Ù£Ù¤Ù¥Ù¦Ù§Ù¨Ù©'
        english_digits = '0123456789'
        
        for text_info in texts:
            text = text_info['text']
            
            # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ø£Ø±Ù‚Ø§Ù… Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©
            arabic_match = re.search(f'[{arabic_digits}]+', text)
            if arabic_match:
                arabic_num = arabic_match.group()
                # ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠ
                english_num = ""
                for char in arabic_num:
                    if char in arabic_digits:
                        english_num += str(arabic_digits.index(char))
                
                if english_num.isdigit():
                    verse_numbers.append({
                        'number': int(english_num),
                        'arabic_text': arabic_num,
                        'x': text_info['x'],
                        'y': text_info['y'],
                        'full_text': text
                    })
            
            # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ø£Ø±Ù‚Ø§Ù… Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ©
            english_match = re.search(f'[{english_digits}]+', text)
            if english_match:
                english_num = english_match.group()
                if english_num.isdigit():
                    verse_numbers.append({
                        'number': int(english_num),
                        'arabic_text': english_num,
                        'x': text_info['x'],
                        'y': text_info['y'],
                        'full_text': text
                    })
        
        return verse_numbers
    
    def match_verses_with_reference_precise(self, extracted_texts, reference_verses):
        """Ù…Ø·Ø§Ø¨Ù‚Ø© Ø§Ù„Ø¢ÙŠØ§Øª Ù…Ø¹ Ø§Ù„Ù…Ø±Ø¬Ø¹ Ø¨Ø¯Ù‚Ø© 100%"""
        results = []
        
        for verse_num, reference_text in reference_verses.items():
            best_match = None
            best_similarity = 0.0
            
            for text_info in extracted_texts:
                similarity = self.font_matcher.calculate_advanced_similarity(
                    text_info['text'], reference_text
                )
                
                if similarity > best_similarity:
                    best_similarity = similarity
                    best_match = text_info
            
            if best_match and best_similarity > 0.8:  # Ø¹ØªØ¨Ø© Ø¹Ø§Ù„ÙŠØ© Ù„Ù„Ø¯Ù‚Ø©
                results.append({
                    'verse_number': verse_num,
                    'reference_text': reference_text,
                    'extracted_text': best_match['text'],
                    'similarity': best_similarity,
                    'coordinates': {
                        'x': best_match['x'],
                        'y': best_match['y']
                    },
                    'match_quality': 'excellent' if best_similarity > 0.95 else 'good'
                })
        
        return results
    
    def process_page_precise(self, svg_file, reference_verses):
        """Ù…Ø¹Ø§Ù„Ø¬Ø© ØµÙØ­Ø© ÙˆØ§Ø­Ø¯Ø© Ø¨Ø¯Ù‚Ø©"""
        print(f"Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ù„Ù: {svg_file}")
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†ØµÙˆØµ
        extracted_texts = self.extract_text_from_svg(svg_file)
        print(f"ØªÙ… Ø§Ø³ØªØ®Ø±Ø§Ø¬ {len(extracted_texts)} Ù†Øµ")
        
        # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø£Ø±Ù‚Ø§Ù… Ø§Ù„Ø¢ÙŠØ§Øª
        verse_numbers = self.find_verse_numbers_precise(extracted_texts)
        print(f"ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ {len(verse_numbers)} Ø±Ù‚Ù… Ø¢ÙŠØ©")
        
        # Ù…Ø·Ø§Ø¨Ù‚Ø© Ø§Ù„Ø¢ÙŠØ§Øª
        matched_verses = self.match_verses_with_reference_precise(extracted_texts, reference_verses)
        print(f"ØªÙ… Ù…Ø·Ø§Ø¨Ù‚Ø© {len(matched_verses)} Ø¢ÙŠØ©")
        
        return {
            'file': svg_file,
            'extracted_texts': extracted_texts,
            'verse_numbers': verse_numbers,
            'matched_verses': matched_verses,
            'statistics': {
                'total_texts': len(extracted_texts),
                'verse_numbers_found': len(verse_numbers),
                'verses_matched': len(matched_verses),
                'accuracy': len(matched_verses) / len(reference_verses) * 100 if reference_verses else 0
            }
        }

def load_reference_verses(reference_file):
    """ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¢ÙŠØ§Øª Ø§Ù„Ù…Ø±Ø¬Ø¹ÙŠØ©"""
    if not os.path.exists(reference_file):
        print(f"Ù…Ù„Ù Ø§Ù„Ù…Ø±Ø¬Ø¹ ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯: {reference_file}")
        return {}
    
    try:
        with open(reference_file, 'r', encoding='utf-8') as f:
            return json.load(f)
    except Exception as e:
        print(f"Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ø±Ø¬Ø¹: {e}")
        return {}

def create_sample_reference():
    """Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø±Ø¬Ø¹ ØªØ¬Ø±ÙŠØ¨ÙŠ Ù„Ù„Ø§Ø®ØªØ¨Ø§Ø±"""
    sample_verses = {
        1: "Ø¨ÙØ³Û¡Ù…Ù Ù±Ù„Ù„ÙÙ‘Ù‡Ù Ù±Ù„Ø±ÙÙ‘Ø­Û¡Ù…ÙÙ°Ù†Ù Ù±Ù„Ø±ÙÙ‘Ø­ÙÙŠÙ…Ù",
        2: "Ù±Ù„Û¡Ø­ÙÙ…Û¡Ø¯Ù Ù„ÙÙ„ÙÙ‘Ù‡Ù Ø±ÙØ¨ÙÙ‘ Ù±Ù„Û¡Ø¹ÙÙ°Ù„ÙÙ…ÙÙŠÙ†Ù",
        3: "Ù±Ù„Ø±ÙÙ‘Ø­Û¡Ù…ÙÙ°Ù†Ù Ù±Ù„Ø±ÙÙ‘Ø­ÙÙŠÙ…Ù",
        4: "Ù…ÙÙ°Ù„ÙÙƒÙ ÙŠÙÙˆÛ¡Ù…Ù Ù±Ù„Ø¯ÙÙ‘ÙŠÙ†Ù",
        5: "Ø¥ÙÙŠÙÙ‘Ø§ÙƒÙ Ù†ÙØ¹Û¡Ø¨ÙØ¯Ù ÙˆÙØ¥ÙÙŠÙÙ‘Ø§ÙƒÙ Ù†ÙØ³Û¡ØªÙØ¹ÙÙŠÙ†Ù",
        6: "Ù±Ù‡Û¡Ø¯ÙÙ†ÙØ§ Ù±Ù„ØµÙÙ‘Ø±ÙÙ°Ø·Ù Ù±Ù„Û¡Ù…ÙØ³Û¡ØªÙÙ‚ÙÙŠÙ…Ù",
        7: "ØµÙØ±ÙÙ°Ø·Ù Ù±Ù„ÙÙ‘Ø°ÙÙŠÙ†Ù Ø£ÙÙ†Û¡Ø¹ÙÙ…Û¡ØªÙ Ø¹ÙÙ„ÙÙŠÛ¡Ù‡ÙÙ…Û¡ ØºÙÙŠÛ¡Ø±Ù Ù±Ù„Û¡Ù…ÙØºÛ¡Ø¶ÙÙˆØ¨Ù Ø¹ÙÙ„ÙÙŠÛ¡Ù‡ÙÙ…Û¡ ÙˆÙÙ„ÙØ§ Ù±Ù„Ø¶ÙÙ‘Ø¢Ù„ÙÙ‘ÙŠÙ†Ù"
    }

    with open("reference_verses.json", 'w', encoding='utf-8') as f:
        json.dump(sample_verses, f, ensure_ascii=False, indent=2)

    print("ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù Ù…Ø±Ø¬Ø¹ÙŠ ØªØ¬Ø±ÙŠØ¨ÙŠ: reference_verses.json")
    return sample_verses

def main():
    """Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©"""
    print("Ù…Ø³ØªØ®Ø±Ø¬ Ø¥Ø­Ø¯Ø§Ø«ÙŠØ§Øª Ø§Ù„Ø¢ÙŠØ§Øª Ø¨Ø¯Ù‚Ø© 100%")
    print("ÙŠØ³ØªØ®Ø¯Ù… Ø¬Ø¯ÙˆÙ„ Ø§Ù„Ø®Ø· Ø§Ù„Ø¹Ø«Ù…Ø§Ù†ÙŠ Ù„Ù„Ù…Ø·Ø§Ø¨Ù‚Ø© Ø§Ù„Ø¯Ù‚ÙŠÙ‚Ø©")
    print("=" * 60)

    # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ù…Ù„Ù Ø§Ù„Ø®Ø·
    font_file = "UthmanicHafs_v2-0 font/uthmanic_hafs_v20.ttf"
    if not os.path.exists(font_file):
        print(f"âŒ Ù…Ù„Ù Ø§Ù„Ø®Ø· ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯: {font_file}")
        return 1
    else:
        print(f"âœ… ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù…Ù„Ù Ø§Ù„Ø®Ø·: {font_file}")

    # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬ Ø§Ù„Ù…ØªÙ‚Ø¯Ù…
    extractor = AdvancedAyatExtractor(font_file)

    # ØªØ­Ù…ÙŠÙ„ Ø£Ùˆ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…Ø±Ø¬Ø¹
    reference_verses = load_reference_verses("reference_verses.json")
    if not reference_verses:
        print("Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø±Ø¬Ø¹ ØªØ¬Ø±ÙŠØ¨ÙŠ...")
        reference_verses = create_sample_reference()

    print(f"ØªÙ… ØªØ­Ù…ÙŠÙ„ {len(reference_verses)} Ø¢ÙŠØ© Ù…Ø±Ø¬Ø¹ÙŠØ©")

    # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ù„ÙØ§Øª
    svg_files = list(Path(".").glob("*.svg"))
    if not svg_files:
        # Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ù…Ø¬Ù„Ø¯ pages_svgs
        svg_files = list(Path("pages_svgs").glob("*.svg"))
        if not svg_files:
            print("Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…Ù„ÙØ§Øª SVG Ù„Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©")
            print("ØªØ£ÙƒØ¯ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ù…Ù„ÙØ§Øª SVG ÙÙŠ Ø§Ù„Ù…Ø¬Ù„Ø¯ Ø§Ù„Ø­Ø§Ù„ÙŠ Ø£Ùˆ Ù…Ø¬Ù„Ø¯ pages_svgs")
            return 1
        else:
            print(f"ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù…Ù„ÙØ§Øª SVG ÙÙŠ Ù…Ø¬Ù„Ø¯ pages_svgs")

    print(f"ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ {len(svg_files)} Ù…Ù„Ù SVG")

    all_results = []
    processed_count = 0

    for svg_file in svg_files[:5]:  # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø£ÙˆÙ„ 5 Ù…Ù„ÙØ§Øª Ù„Ù„Ø§Ø®ØªØ¨Ø§Ø±
        try:
            result = extractor.process_page_precise(str(svg_file), reference_verses)
            all_results.append(result)
            processed_count += 1

            print(f"Ø§Ù„Ù…Ù„Ù: {svg_file.name}")
            print(f"  Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬Ø©: {result['statistics']['total_texts']}")
            print(f"  Ø£Ø±Ù‚Ø§Ù… Ø§Ù„Ø¢ÙŠØ§Øª: {result['statistics']['verse_numbers_found']}")
            print(f"  Ø§Ù„Ø¢ÙŠØ§Øª Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚Ø©: {result['statistics']['verses_matched']}")
            print(f"  Ø¯Ù‚Ø© Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚Ø©: {result['statistics']['accuracy']:.1f}%")
            print("-" * 40)

        except Exception as e:
            print(f"Ø®Ø·Ø£ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© {svg_file}: {e}")
            continue

    if not all_results:
        print("Ù„Ù… ÙŠØªÙ… Ù…Ø¹Ø§Ù„Ø¬Ø© Ø£ÙŠ Ù…Ù„Ù Ø¨Ù†Ø¬Ø§Ø­")
        return 1

    # Ø­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
    output_file = "precise_extraction_results.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(all_results, f, ensure_ascii=False, indent=2)

    print(f"ØªÙ… Ø­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ÙÙŠ: {output_file}")

    # Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø´Ø§Ù…Ù„Ø©
    total_accuracy = sum(r['statistics']['accuracy'] for r in all_results) / len(all_results)
    total_verses_matched = sum(r['statistics']['verses_matched'] for r in all_results)
    total_texts_extracted = sum(r['statistics']['total_texts'] for r in all_results)

    print(f"\n{'='*50}")
    print(f"Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©:")
    print(f"{'='*50}")
    print(f"Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©: {processed_count}")
    print(f"Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬Ø©: {total_texts_extracted}")
    print(f"Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ø¢ÙŠØ§Øª Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚Ø©: {total_verses_matched}")
    print(f"Ø§Ù„Ø¯Ù‚Ø© Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠØ©: {total_accuracy:.1f}%")

    if total_accuracy >= 95:
        print("ğŸ‰ Ø¯Ù‚Ø© Ù…Ù…ØªØ§Ø²Ø©! Ø§Ù„Ø³ÙƒØ±ÙŠØ¨Øª ÙŠØ¹Ù…Ù„ Ø¨ÙƒÙØ§Ø¡Ø© Ø¹Ø§Ù„ÙŠØ©")
    elif total_accuracy >= 80:
        print("âœ… Ø¯Ù‚Ø© Ø¬ÙŠØ¯Ø©ØŒ ÙŠÙ…ÙƒÙ† ØªØ­Ø³ÙŠÙ†Ù‡Ø§ Ø£ÙƒØ«Ø±")
    else:
        print("âš ï¸ Ø§Ù„Ø¯Ù‚Ø© Ù…Ù†Ø®ÙØ¶Ø©ØŒ ÙŠØ­ØªØ§Ø¬ ØªØ­Ø³ÙŠÙ†")

    return 0

if __name__ == "__main__":
    exit(main())
